#### ConcurrentHashMap在JDK1.7中的实现方式




#### ConcurrentHashMap在JDK1.8中实现方式
ConcurrentHashMap没有使用JDK1.7中的Segment分段锁，采用了CAS+Synchronized来保证并发的安全性
其中val next都用volatile修饰，保证了可见性。

什么是CAS
底层是使用Unsafe包中native code实现的。
CAS有三个数：内存值V,旧的预期值A,要修改的新值B,仅当预期值A和内存值V相同时，才能将内存值修改为B,否则什么都不做。

CAS的缺点：ABA问题

如何解决ABA问题：对变量增加一个版本号，每次修改，版本号加1，比较的时候比较版本号

put过程
根据key计算出hashcode
判断是否需要进行初始化
通过key定位出的Node，如果为null表示当前位置可以写入数据，利用CAS尝试写入，失败则自旋保证成功
如果当前位置的hashcode==MOVED==-1，则需要进行扩容
如果都不满足，则利用synchronized锁写入数据
如果数量大于TREEIFY_THRESHOLD则要转换为红黑树



get过程
根据计算出来的hashcode寻址，如果就在桶上那么直接返回值
如果是红黑树那么就按照树的方式获取值
都不满足那就按照链表的方式遍历获取值

### HashMap什么时候需要重写Hashcode和equals方法？
当key值为对象的时候，必须要求重写hashCode()和equal()。因为默认的hashcode()使用的是该值在内存中的地址作为该值的hash值，如果两个具有相同意义的对象进行比较时，由于其地址的不同会导致计算出的hash值也不同，而重写equals()可以确保两个对象具有相同意义的属性。

### HashMap中的hash函数是怎么实现的？
```java
static final int hash(Object key){
	int h;
	return (key==null) ? 0:(h=key.hashCode())^(h>>16);
}
//hash通过计算key自身的hashCode()值，返回的类型是int,为了减少碰撞产生的几率，将获得的hashCode()的低16位与高16位右移16位相异或。
```
### 为什么HashMap是线程不安全的？如何体现出不安全的？
当多个线程对HashMap进行put操作时，如果put的key相同会产生碰撞，那么HashMap会将两个key放在数组的同一位置，其中一个线程key会被覆盖掉

当多个线程检测到数组需要扩容时，都会进行数组中元素hash值的重新计算和数据复制，那么也势必会造成最后只有一个线程创建的数组成功赋值给table

### HashMap是如何扩容的？
扩容时新建一个HashMap的底层数组，然后调用transfer方法，将HashMap中的全部元素添加到新的HashMap中（需要重新计算元素在新的数组中的索引位置）。很明显，扩容是一个相当耗时的操作，因为它需要重新计算这些元素在新的数组中的位置并进行复制处理，因此我们在用HashMap的时候，最好能提前预估下HashMap中元素的个数，这样有助于提高hashMap的性能
```java
void resize(int newCapacity) {
       Entry[] oldTable = table;
       int oldCapacity = oldTable.length;
       if (oldCapacity == MAXIMUM_CAPACITY) {
           threshold = Integer.MAX_VALUE;
           return;
       }

       Entry[] newTable = new Entry[newCapacity];
       //初始化一个大小为oldTable容量两倍的新数组newTable
       transfer(newTable);
       table = newTable;
       threshold = (int)(newCapacity * loadFactor);
   }
   //关于HashMap扩容最关键的一步操作是transfer(newTable),这个操作会把当前Entry[]table数组的全部元素转移到新的table中，这个transfer的过程再并发环境下会发生错误，导致数组链表中的链表形成循环链表，在后面的get操作时候e=e.next操作无线循环
```


### ConcurrentHashMap与Hashtable
HashTable采用Synchronized同步实现，对其中所有方法添加监视器锁，问题在于，在一个线程进行put操作时候，其他线程无法获得get或者其他操作，而concurrentHashMap采用分段锁技术。

### 为什么String、Integer这样的wrapper类适合作为健？
HashMap推荐使用不可变变量作为key，其中String最为常用，因为String时不可变的，也是final的，我们通过计算key的hash值在数组中进行查找，如果key是可变类型，那么在插入和获取时返回的hashcode就不同，就无法正确获得我们想要的对象

### 我们可以使用自定义的对象作为key么？
可以，但是要确保这个对象重写了hashCode()和equal()方法，并且该对象插入Map后就不再改变，只要遵循了这些，它就可以作为key了


### HashMap的工作原理，其中get()方法的工作原理？（或者：HashMap底层数据结构？）
HashMap是基于hash原理，通过put()和get()方法存储和获取元素，它内部使用数组+链表或红黑树的结构，通过hash运算找到bucket的位置来存储Entry对象，通过equals()方法找到正确的键值对。HashMap使用链地址法来解决hash碰撞问题，当发生碰撞时，对象会存储在链表的下一个节点。

get()
1、首先会通过计算key的hash值找到在数组中的下标，
2、若下标处元素为空则返回null
3、否则判断该下标处元素是否是要查找的元素，是就直接返回，不是就判断该元素节点是否为红黑树节点，是就进行红黑树节点的查找，否则遍历链表进行查找。
关键点：HashMap在bucket中存储键对象和值作为 Map.Entry

put()
1、计算key 的hash值，算出元素在底层数组中的下标位置。
2、通过下标位置定位到底层数组里的元素（也有可能个是链表也有可能是树）。
3、取到元素，判断放入的元素的key是否==或equals当前位置的key，成立则替换树里的value,并返回旧值。

HashMap底层数据结构：HashMap就是数组+链表的组合实现的，每个数组元素存储一个链表的头结点，本质上来说是哈希表"拉链法"的实现。
如图：

在jdk1.8中HashMap的实现方法做了一些改变，但是基本思想还是没有变得，只是再一些地方做了优化，数据结构的存储由数组+链表的方式，变化为数组+链表+红黑树的存储方式，在性能上进一步得到提升。

如图：


### put方法原理？

put源码分析
```java
public V put(K key, V value) {
    //调用putVal()方法完成
    return putVal(hash(key), key, value, false, true);
}

final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
               boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    //判断table是否初始化，否则初始化操作
    if ((tab = table) == null || (n = tab.length) == 0)
        n = (tab = resize()).length;
    //计算存储的索引位置，如果没有元素，直接赋值
    if ((p = tab[i = (n - 1) & hash]) == null)
        tab[i] = newNode(hash, key, value, null);
    else {
        Node<K,V> e; K k;
        //节点若已经存在，执行赋值操作
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            e = p;
        //判断链表是否是红黑树
        else if (p instanceof TreeNode)
            //红黑树对象操作
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        else {
            //为链表，
            for (int binCount = 0; ; ++binCount) {
                if ((e = p.next) == null) {
                    p.next = newNode(hash, key, value, null);
                    //链表长度8，将链表转化为红黑树存储
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        treeifyBin(tab, hash);
                    break;
                }
                //key存在，直接覆盖
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                p = e;
            }
        }
        if (e != null) { // existing mapping for key
            V oldValue = e.value;
            if (!onlyIfAbsent || oldValue == null)
                e.value = value;
            afterNodeAccess(e);
            return oldValue;
        }
    }
    //记录修改次数
    ++modCount;
    //判断是否需要扩容
    if (++size > threshold)
        resize();
    //空操作
    afterNodeInsertion(evict);
    return null;
}
```
下面对源码中的流程做一个总结：
1、计算key 的hash值，算出元素在底层数组中的下标位置。
2、通过下标位置定位到底层数组里的元素（也有可能个是链表也有可能是树）。
3、取到元素，判断放入的元素的key是否==或equals当前位置的key，成立则替换树里的value,并返回旧值。



### HashMap与HashTable的区别？


### ConcurrentHashMap是如何实现并发的？


### 关于HashMap中的哈希冲突（哈希碰撞）以及冲突解决办法？
HashMap通过计算key的hash值来确定数组位置，不同的key可能会产生相同hash值，因此会发生hash碰撞，在HashMap中采用链地址法解决Hash冲突，当发生碰撞时，将相同hash值的元素存储在链表的下一个节点处。

### 如果HashMap的大小超过负载因子定义的容量会怎么办？
HashMap中默认的负载因子为0.75，当HashMap中数组元素个数超过负载因子乘以数组总容量时会发生扩容，将数组扩展为原来的两倍。对原数组中元素再进行hash运算，将获得再新数组中的位置。



### HashMap在并发下可能出现的问题分析？
https://coolshell.cn/articles/9606.html
JDK1.7 HashMap在并发环境下多线程put后（put后多线程扩容transfer）可能get死循环，具体表现为CPU利用率100%
```java
//假设第一个线程执行到这里因为某种原因挂起
    Entry next = e.next;
    int i = indexFor(e.hash, newCapacity);
    e.next = newTable[i];
    newTable[i] = e;
    e = next;
```
在多线程环境下put的时候可能导致元素丢失（1.8和1.7中都有）
在多线程put操作时候，执行addEntry(hash，key,value ,i)如果产生哈希碰撞，导致两个线程得到同样的bucketindex去存储，就可能出现覆盖丢失的情况：
```java
void addEntry(int hash, K key, V value, int bucketIndex) {
    //多个线程操作数组的同一个位置
    Entry e = table[bucketIndex];
        table[bucketIndex] = new Entry(hash, key, value, e);
        if (size++ >= threshold)
            resize(2 * table.length);
    }
```
### Q: HashMap的工作原理
> A: HashMap底层是hash数组和单向链表及红黑树实现，数组中的每个元素都是链表或者红黑树，HashMap通过put&get方法存储和获取
> 存储对象时，将Key/value值传给put()方法：
> - 调用hash(key)方法计算key的hash值，然后结合数组长度，计算得到数组的下标
> - 调整数组大小（但容器中元素个数大于capacity*loadfactor时，容器会进行扩容resize的2n）
> - 如果计算出的该哈希桶的位置没有值，则把新插入的key-value放到此处
>           if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
> - 计算出的该哈希桶的位置有值，出现哈希冲突的几种情况：
>     + 第一种：插入的key的hash值，key都与当前节点(头结点)的相等
>     + 第二种：key hash值不等于头结点，判断该p是否属于红黑树的节点，并在红黑树中添加该节点
>     + 第三种：key hasH值不等于首节点，不为红黑树的节点，则为链表节点，则遍历链表
>         * 如果到链表尾部，则直接插入
>         * 如果在未遍历到链表尾部之前出现重复的key，则覆盖value
>         * (JDK1.7 之前使用头插法、JDK1.8使用尾插法)
>         * （注意：当碰撞导致链表大于 TREEIFY_THRESHOLD = 8 时，就把链表转换成红黑树）
> + 获取对象时：
>     * 如果是头结点，则是直接返回头结点
>     * 如果不是头结点，则判断是否还有下一个节点，对后面的节点进行遍历判断
>     * 判断该节点是否是树中的一个节点，则去树中找并返回该节点
>     * 如果该节点是链表，则遍历链表寻找该节点
>         - 找到并返回
>         - 遍历到链表尾部，就进行终止，返回null
>
### Q: 你知道Hash的实现么？为什么要这么实现？
A： 在JDK1.8中，是通过hashCode()的高16位异或低16位实现的：
（h=key.hashCode()）^ (h>>16)
主要是从速度，功效和质量来考虑的，减少系统的开销，也不会造成因为高位没有参与下标运算，从而引起的碰撞

### Q：为什么要用异或运算
A: 保证了对象的hashCode的32位值只要有一位发生改变，整个hash()返回值就会改变，尽可能的减少碰撞。

### Q：HashMap的table的容量如何确定？loadFactor是什么？该容量如何变化？这种变化会带来什么问题？
A：

- table数组大小是由capacity这个参数确定的，默认是16
- loadFactor是装载因子，主要是用来确认table数组是否需要动态扩展，默认值是0.75，比如table 数组大小为 16，装载因子为 0.75 时，threshold 就是12，当 table 的实际大小超过 12 时，table就需要动态扩容；
- 扩容时，调用resize()方法，将table长度变为原来的两倍
- 如果数据很大的情况下，扩展时将会带来性能的损失，在性能要求很高的地方，这种损失可能是致命的。



### 源码解析：JDK版本1.8
```java
阅读源码前，需要对如下有熟悉
 * 关键字：static、final、transient、instanceof
 * 运算符：<< 、 ^ 、  >>> 、  |=
 *
 * 回顾基础：
 *
 * static：
 * 1.被static修饰的变量或者方法是独立于该类的任何对象，也就是说，这些变量和方法不属于任何一个实例对象，而是被类的实例对象所共享，而非静态变量是对象所拥有的。
 * 2.在类被加载的时候，就会去加载被static修饰的部分。
 * 3.被static修饰的变量或者方法是优先于对象存在的，也就是说当一个类加载完毕之后，即便没有创建对象，也可以去访问。
 * 4.static方法就是没有this的方法
 * 5.在static方法内部不能调用非静态方法
 * 6.在静态方法中不能访问非静态成员方法和非静态成员变量，因为非静态成员方法/变量都是须依赖具体的对象才能够被调用。但是在非静态成员方法中是可以访问静态成员方法/变量的。
 * 7.static关键字并不会改变变量和方法的访问权限，静态成员变量虽然独立于对象，但是不代表不可以通过对象去访问，所有的静态方法和静态变量都可以通过对象访问（只要访问权限足够）。
 * 8.static是不允许用来修饰局部变量。
 * 9.static修饰的属性和方法，使用类名.xx的形式访问。
 * 10.使用的位置：那些不需要通过创建对象就可以访问方法的情况。
 *
 *
 * final:
 * 1.修饰类当用final去修饰一个类的时候，表示这个类不能被继承。
 * 2.被final修饰的类，final类中的成员变量可以根据自己的实际需要设计为fianl。
 * 3.final类中的成员方法都会被隐式的指定为final方法。
 * 4.被final修饰的方法不能被重写。
 * 5.一个类的private方法会隐式的被指定为final方法。
 * 6.如果父类中有final修饰的方法，那么子类不能去重写。
 * 7.修饰成员变量必须要赋初始值，而且是只能初始化一次。
 * 8.修饰成员变量必须初始化值。被fianl修饰的成员变量赋值，有两种方式：1、直接赋值 2、全部在构造方法中赋初值。
 * 9.如果修饰的成员变量是基本类型，则表示这个变量的值不能改变。
 * 10.如果修饰的成员变量是一个引用类型，则是说这个引用的地址的值不能修改，但是这个引用所指向的对象里面的内容还是可以改变的。
 *
 *
 * transient:
 * 1.在已序列化的类中使变量不序列化——在已实现序列化的类中，有的变量不需要保存在磁盘中，就要transient关键字修饰.
 * 2.transient只能修饰成员变量，不能修饰方法。
 *
 * instanceof：
 * 在运行时指出的对象是否是特定类的一个实例
 *
 * <<（左移位运算） ：(>>：则反过来)
 * 1<<2 = 2 、  1 << 3 = 8 、1 << 4 = 16 ......
 * 20的二进制补码：0001 0100
 * 向左移动两位后：0101 0000  》》》   20 << 2
 *
 *
 * ^(异或运算符)：
 * 5^9 = 12 》》》 5的二进制位是0000 0101 ， 9的二进制位是0000 1001，也就是0101 ^ 1001,结果为1100，00001100的十进制位是12
 *
 * >>>（无符号右移运算符）：
 * 无符号右移运算符和右移运算符的主要区别在于负数的计算，因为无符号右移是高位补0，移多少位补多少个0。
 * 15的二进制位是0000 1111 ， 右移2位0000 0011，结果为3
 *
 *
 * |=（运算符，先计算|再=）：
 * n|=n>>>2 》》》  n=n|(n>>>2)=1000100|0010001=1010101
 * */


public class HashMap<K,V> extends AbstractMap<K,V>
    implements Map<K,V>, Cloneable, Serializable {

    private static final long serialVersionUID = 362498820763181265L;  //版本序列号

    /**
     * 文本翻译：
     *
     * 实现注意事项：
     *
     * 1.桶上的红黑树主要是按hashCode排序，如果来个元素相同，则通过compareTo方法进行比较排序。
     * 2.当普通节点数大于TREEIFY_THRESHOLD时，就会进行大小调整成红黑树结构，树节点的大小大约是普通节点的两倍，当树节点太小的时候也会转换为普通节点大小。
     * 3.默认大小调整的参数平均约为0.5，阈值为0.75忽略方差，得到期望列表大小k的出现次数为(exp(-0.5) * pow(0.5, k) / factorial (k))。第一个值是:
     * 0:    0.60653066
     * 1:    0.30326533
     * 2:    0.07581633
     * 3:    0.01263606
     * 4:    0.00157952
     * 5:    0.00015795
     * 6:    0.00001316
     * 7:    0.00000094
     * 8:    0.00000006
     * more: less than 1 in ten million
     *
     * 4.树的根节点为第一个节点，如果该节点被删除，可以通过TreeNode.root()方法来恢复根节点。
     * 5.所有适用的内部方法都接受哈希码作为参数(通常由公共方法提供)，允许在不重新计算用户哈希码的情况下相互调用。大多数内部方法也接受“tab”参数，也就是说通常是当前表，但可能是新表或旧表时调整大小或转换
     * 6.当bin列表被treeified、split或untreeified时，我们保存以相同的相对存取/遍历次序(即、现场为了更好地保存局部，并稍微简化对调用的分割和遍历的处理iterator.remove。
     * 7.当使用比较器插入时，要保持跨区域的总排序(或与此处所需的最接近)rebalancings，我们将类和identityhashcode进行比较参加。
     * 8.在普通模式和树模式之间的使用和转换是由于LinkedHashMap子类的存在而变得复杂。看到下面是定义在插入时调用的钩子方法，删除和访问允许LinkedHashMap内部文件否则独立于这些机制。
     * 9.这也要求将映射实例传递给一些实用程序方法可能会创建新节点。
     * 10.基于并行编程的类似于ssa的编码风格很有帮助避免所有扭曲指针操作中的混叠错误。
     * */
    static final int DEFAULT_INITIAL_CAPACITY = 1 << 4;    //默认初始容量—必须是2的幂。值为16。

    static final int MAXIMUM_CAPACITY = 1 << 30;  //最大容量限制：2的30次方

    static final float DEFAULT_LOAD_FACTOR = 0.75f;    //构造函数中没有指定时，默认使用的负载因子= 0.75f

    static final int TREEIFY_THRESHOLD = 8;   //当桶(bucket)上的结点数大于这个值时会转成红黑树

    static final int UNTREEIFY_THRESHOLD = 6;  //当桶(bucket)上的结点数小于这个值时树转链表，前提是它当前是红黑树结构。

    static final int MIN_TREEIFY_CAPACITY = 64;    //当整个hashMap中元素数量大于64时，也会进行转为红黑树结构。

    //单向链表，每个节点都存有hash值，key-value，next指针。
    static class Node<K,V> implements Entry<K,V> {
        final int hash;
        final K key;
        V value;
        Node<K,V> next;

        //构造一个节点
        Node(int hash, K key, V value, Node<K,V> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }

        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + "=" + value; }

        //重写方法，返回对该节点元素计算出来的hashCode。
        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }

        //允许为value重新设置新值
        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }

        //重写方法，判断是否能找到该节点对象
        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Entry<?,?> e = (Entry<?,?>)o;
                if (Objects.equals(key, e.getKey()) && Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
    }

    //计算key的hash码
    /* ---------------- Static utilities -------------- */


    static final int hash(Object key) {
        int h; //哈希码
        /**
         *  如果key==null哈希值就返回0，否则就计算出前的key的哈希码与哈希码无符号右移16位，后移的每一个位置都补零，接着进行异或（掩码），得出该key的哈希码
         *  计算key.hashCode()并传播(XORs)更高的散列位，因为表使用的是2的幂掩码，所以是只在当前掩码上方的位上变化的散列会总是发生碰撞。
         *  高16位与低16位异或来减少这种碰撞影响，这样操作与table下标的计算有关：n = table.length; index = （n-1） & hash;
         *  table的长度都是2的幂，因此index仅与hash值的低n位有关（此n非table.leng，而是2的幂指数），hash值的高位都被与操作置为0了。
         *
         *【1】计算过程：
         *
         *16的二进制为：10000
         *15的二进制为：1111
         *
         *假如有这两个对象的hashCode:
         *对象A：1000010001110001000001111000000
         *两个数进行与异或
         *对象B：0111011100111000101000010100000
         *15与对象A和对象B进行异或后的结果进行&操作后结果都是0
         *解决方法：
         *1.将hashCode右移16位，也就是取int类型的一半，刚好将二进制数对半切开，然后使用异或。（如果两个数对应的位置相反，则结果为1，反之为0），这样就能避免上面的情况发生。
         *
         *在一些极端情况下还是有问题，比如：
     *10000000000000000000000000 和  1000000000100000000000000
     *这两个数，如果数组长度是16，那么即使右移16位，再异或，hash值还是会重复。但是为了性能，对这种极端情况，JDK的作者选择了性能。毕竟这是少数情况，为了这种情况去增加 hash时间，性价比不高。
         * */
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }

    //如果它的形式是：class C implements Comparable<C>，返回x的类，否则为空。
    static Class<?> comparableClassFor(Object x) {

        if (x instanceof Comparable) {//判断指出的对象是否是特定类的一个实例
            Class<?> c; //定义实例类
            Type[] ts, as; //定义数组类型
            Type t; //定义原始类型
            ParameterizedType p; //定义参数化类型
            if ((c = x.getClass()) == String.class)    //找到x的类，直接返回
                return c;
            if ((ts = c.getGenericInterfaces()) != null) { //如果该类的类型数组参数不为空
                for (int i = 0; i < ts.length; ++i) { //遍历该类的类型数组
                  //如果该类的类型符合一系列参数类型和类型对象条件，则返回该类
                    if (((t = ts[i]) instanceof ParameterizedType) && ((p = (ParameterizedType)t).getRawType() == Comparable.class) && (as = p.getActualTypeArguments()) != null && as.length == 1 && as[0] == c)
                        return c;
                }
            }
        }
        return null;
    }


    @SuppressWarnings({"rawtypes","unchecked"})
    static int compareComparables(Class<?> kc, Object k, Object x) {
      //如果x匹配kc，k是可比较的，返回k.compareto(x)，否则返回0。
        return (x == null || x.getClass() != kc ? 0 : ((Comparable)k).compareTo(x));
    }

    //算法1：通过移位和减法来代替乘法，与2相乘等价于移位运算(低位补0)，例如：31*i == (i<<5)-1
    static final int tableSizeFor(int cap) {  //返回一个比给定整数大且最接近的2的幂次方整数。
      //防止现在就是2的幂次，如果不减，经过下面的算法会把最高位后面的都置位1，再加1则相当于将当前的数值乘2
      /**
       * 这个与HashMap中table下标的计算有关。
       * n = table.length;
     * index = （n-1） & hash;
     * 因为，table的长度都是2的幂，因此index仅与hash值的低n位有关（此n非table.leng，而是2的幂指数），hash值的高位都被与操作置为0了。
     *
     * 【2】为什么要n-1呢？
     *
     * 当 n为 2 的幂次方的时候，减一之后就会得到 1111*的数字，这个数字正好可以掩码。
     * 并且得到的结果取决于 hash值。因为 hash值是1，那么最终的结果也是1 ，hash值是0，最终的结果也是0。
       * */
        int n = cap - 1;
        n |= n >>> 1;//第一次右移
        n |= n >>> 2;//第二次右移，这里的n是上面式子计算出来的结果，后面以此类推
        n |= n >>> 4;//第三次右移
        n |= n >>> 8;//第四次右移
        /**
         * h >>> 16，表示无符号右移16位，高位补0，任何数跟0异或都是其本身，因此key的hash值高16位不变。
         * */
        n |= n >>> 16;//第五次右移、key的hash值高16位不变，低16位与高16位异或作为key的最终hash值。
        //如果n<0就返回1，否则如果n是大于等于最大容量限制就返回最大容量值，不然返回n+1。
        return (n < 0) ? 1 : (n >= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;
    }

    /**
     *【3】HashMap 的容量为什么建议是 2的幂次方？
     * hash 算法的目的是为了让hash值均匀的分布在桶中（数组），如果不使用 2 的幂次方作为数组的长度会怎么样？
     * 假设我们的数组长度是10，还是上面的公式：
     * 1010 & 101010100101001001000 结果：1000 = 8
     * 1010 & 101000101101001001001 结果：1000 = 8
     * 1010 & 101010101101101001010 结果：1010 = 10
     * 1010 & 101100100111001101100 结果：1000 = 8
     * 这种散列结果，会导致这些不同的key值全部进入到相同的插槽中，形成链表，性能急剧下降。
     * 所以一定要保证 &中的二进制位全为 1，才能最大限度的利用 hash 值，并更好的散列，只有全是1，才能有更多的散列结果。
     * 如果是 1010，有的散列结果是永远都不会出现的，比如 0111，0101，1111，1110…，只要 &之前的数有 0， 对应的 1肯定就不会出现（因为只有都是1才会为1）。
     * 大大限制了散列的范围。
     *
     * 【4】自定义 HashMap 容量最好是多少？
     * 如果Map中已有数据的容量达到了初始容量的 75%，那么散列表就会扩容，而扩容将会重新将所有的数据重新散列，性能损失严重.
     * 如果是2个数据的话可以将初始化容量设置为4。如果不是2的幂次方，散列结果将会大大下降。
     * 如果你预计大概会插入 12 条数据的话，那么初始容量为16简直是完美，一点不浪费，而且也不会扩容。
     * */
    transient Node<K,V>[] table;  //存储元素的数组，总是2的幂次倍

    transient Set<Entry<K,V>> entrySet;    // 存放具体元素的集

    transient int size;      // 存放元素的个数，注意这个不等于数组的长度。

    transient int modCount;    // 每次扩容和更改map结构的计数器

    /**
     * threshold = capacity * loadFactor，当Size>=threshold的时候，
     * 那么就要考虑对数组的扩增了，也就是说，这个的意思就是衡量数组是否需要扩增的一个标准。
     * */
    int threshold;      // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容

    /**
     * loadFactor加载因子是控制数组存放数据的疏密程度，loadFactor越趋近于1，
     * 那么 数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，load Factor越小，也就是趋近于0。
     * loadFactor太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor的默认值为0.75f是官方给出的一个比较好的临界值。
     * */
    final float loadFactor;    // 填充因子

    //HashMap使用指定的初始容量和加载因子构造
    public HashMap(int initialCapacity, float loadFactor) {
        if (initialCapacity < 0)
            throw new IllegalArgumentException("Illegal initial capacity: " +
                                               initialCapacity);
        if (initialCapacity > MAXIMUM_CAPACITY)
            initialCapacity = MAXIMUM_CAPACITY;
        if (loadFactor <= 0 || Float.isNaN(loadFactor))
            throw new IllegalArgumentException("Illegal load factor: " +
                                               loadFactor);
        this.loadFactor = loadFactor;
        this.threshold = tableSizeFor(initialCapacity);//用户自定义的initialCapacity
    }

    //HashMap使用指定的初始容量和默认加载因子（0.75）构造
    public HashMap(int initialCapacity) {
        this(initialCapacity, DEFAULT_LOAD_FACTOR);
    }

    //HashMap使用默认初始容量（16）和默认加载因子（0.75）构造
    public HashMap() {
        this.loadFactor = DEFAULT_LOAD_FACTOR;
    }

    /**HashMap(Map<? extends K, ? extends V> m)  -》   putMapEntries  -》  resize()*/

    //传入一个Map,然后转化成hashMap。
    public HashMap(Map<? extends K, ? extends V> m) {
        this.loadFactor = DEFAULT_LOAD_FACTOR;
        putMapEntries(m, false);//转化方法
    }


    final void putMapEntries(Map<? extends K, ? extends V> m, boolean evict) {
      //得到用户传入的map的长度
        int s = m.size();
        if (s > 0) {
            if (table == null) {    //判断table是否初始化
              /**
               * 求出需要的容量，因为实际使用的长度=容量*0.75得来的，+1是因为小数相除，基本都不会是整数，容量大小不能为小数的，后面转换为int，多余的小数就要被丢掉，所以+1，
               * 例如，map实际长度22，22/0.75=29.3,所需要的容量肯定为30，有人会问如果刚刚好除得整数呢，除得整数的话，容量大小多1也没什么影响
               **/
                float ft = ((float)s / loadFactor) + 1.0F;
                //判断该容量大小是否超出上限，并选择合适的值
                int t = ((ft < (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY);
                //超过链接值，则调用tableSizeFor，返回一个比t大且最接近的2的幂次方整数。
                if (t > threshold)
                    threshold = tableSizeFor(t);
            }
            //如果table已经初始化，则进行扩容操作，resize()就是扩容。
            else if (s > threshold)
                resize();
            //Map中的每一个key-value就是一个Entry实例，遍历取得key-value
            for (Entry<? extends K, ? extends V> e : m.entrySet()) {
                K key = e.getKey();
                V value = e.getValue();
                //放入hashMap中
                putVal(hash(key), key, value, false, evict);
            }
        }
    }

    //返回此映射中键 - 值映射的数量。
    public int size() {
        return size;
    }

    //如果此映射不包含键-值映射关系,返回true
    public boolean isEmpty() {
        return size == 0;
    }

    /**get(Object key) -》   getNode(int hash, Object key)*/

    //通过key获取value，如果是null，则返回null
    public V get(Object key) {
        Node<K,V> e;
        //调用getNode来完成获取
        return (e = getNode(hash(key), key)) == null ? null : e.value;
    }


    final Node<K,V> getNode(int hash, Object key) {
        Node<K,V>[] tab; //hash节点数组
        Node<K,V> first, e; //头结点与临时变量
        int n; //长度
        K k; //key
        //如果数组有元素且数组长度大于0且头结点下标不为空
        if ((tab = table) != null && (n = tab.length) > 0 && (first = tab[(n - 1) & hash]) != null) {
          //如果是头结点，则直接返回头结点
            if (first.hash == hash && ((k = first.key) == key || (key != null && key.equals(k))))  return first;
            //如果不是头结点，就判断是否还有下一个节点，对后面的节点进行遍历判断
            if ((e = first.next) != null) {
              //判断该节点是否是树中的一个节点，如果是就去树中找并返回该节点
                if (first instanceof TreeNode)  return ((TreeNode<K,V>)first).getTreeNode(hash, key);
                do {//链表节点，遍历链表找到该节点，并返回
                    if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k))))   return e;

                } while ((e = e.next) != null); //遍历到最后，就进行终止
            }
        }
        return null;//找不到该节点返回null
    }

    //如果此映射包含指定键的映射，则返回true
    public boolean containsKey(Object key) {
        return getNode(hash(key), key) != null;
    }

    /**put(K key, V value) ——》    putVal() -》 treeifyBin*/

    //向hashmap中添加key-value实例
    public V put(K key, V value) {
      //调用putVal方法
        return putVal(hash(key), key, value, false, true);
    }

    /**
     * onlyIfAbsent：如果该key存在值，如果为null的话，则插入新的value
     * evict：：如果为false，则该表处于创建模式。
     * */
    final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {

        Node<K,V>[] tab; //hash桶
        Node<K,V> p;  //桶中的头节点
        int n, i;  //n hashMap的长度，i 计算出的数组下标

        //获取长度并进行扩容，使用的是懒加载，table一开始是没有加载的，等put后才开始加载
        if ((tab = table ) == null || (n = tab.length) == 0)
            n = (tab = resize()).length;

        /**如果计算出的该哈希桶的位置没有值，则把新插入的key-value放到此处，此处就算没有插入成功，也就是发生哈希冲突时也会把哈希桶的首节点赋予p**/
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
        //哈希冲突的几种情况
        else {
            Node<K,V> e; //临时节点
            K k;   //存放该当前节点的key

            //第一种，插入的key-value的hash值，key都与当前节点的相等
            if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k))))
                e = p; //令其表示为头节点

            else if (p instanceof TreeNode) //第二种，hash值不等于首节点，判断该p是否属于红黑树的节点

              /**为红黑树的节点，则在红黑树中进行添加，如果该节点已经存在，则返回该节点（不为null），该值很重要，用来判断put操作是否成功，如果添加成功返回null**/
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);

            else {//第三种，hash值不等于首节点，不为红黑树的节点，则为链表的节点
                for (int binCount = 0; ; ++binCount) {//遍历链表
                    if ((e = p.next) == null) {//如果找到尾部，则表明添加的key-value没有重复，在尾部进行添加
                        p.next = newNode(hash, key, value, null);
                        if (binCount >= TREEIFY_THRESHOLD - 1)   //判断是否要转换为红黑树结构
                            treeifyBin(tab, hash);
                        break;
                    }
                    //如果链表中有重复的key，e则为当前重复的节点，结束循环
                    if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            //有重复的key，则用待插入值进行覆盖，返回旧值。
            if (e != null) {
                V oldValue = e.value;
                //如果不存在该值或者原来的值为null
                if (!onlyIfAbsent || oldValue == null)
                  //为该节点设置value
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }

        ++modCount; //修改次数+1

        //实际长度+1，判断是否大于临界值，大于则扩容
        if (++size > threshold)
            resize();
        afterNodeInsertion(evict);

        //到了此步骤，则表明待插入的key-value是没有key的重复，因为插入成功e节点的值为null
        return null;
    }

    //扩容方法
    final Node<K,V>[] resize() {
      //把没插入之前的哈希数组做为oldTal
        Node<K,V>[] oldTab = table;
        //old的长度
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        //old的临界值
        int oldThr = threshold;
        //初始化长度与临界值
        int newCap, newThr = 0;
        if (oldCap > 0) {//oldCap > 0也就是说不是首次初始化，因为hashMap用的是懒加载
            if (oldCap >= MAXIMUM_CAPACITY) {//大于最大值
                threshold = Integer.MAX_VALUE;//临界值为整数的最大值
                return oldTab;
            }
            //标记##其它情况，扩容两倍，并且扩容后的长度要小于最大值，old长度也要大于16
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
                newThr = oldThr << 1;    //临界值也扩容为old的临界值2倍
        }
        /**
         * 如果oldCap<0，但是已经初始化了，像把元素删除完之后的情况，那么它的临界值肯定还存在，
         * 如果是首次初始化，它的临界值则为0
         **/
        else if (oldThr > 0)
            newCap = oldThr;
        //首次初始化，给与默认的值
        else {
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//临界值等于容量*加载因子
        }
        if (newThr == 0) {//此处的if为上面标记##的补充，也就是初始化时容量小于默认值16的，此时newThr没有赋值
            float ft = (float)newCap * loadFactor;//new的临界值
            //判断是否new容量是否大于最大值，临界值是否大于最大值
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        //把上面各种情况分析出的临界值，在此处真正进行改变，也就是容量和临界值都改变了。
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
        //初始化
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        table = newTab; //赋予当前的table

        //此处自然是把old中的元素，遍历到new中
        if (oldTab != null) {
            for (int j = 0; j < oldCap; ++j) {
                //临时变量
                Node<K,V> e;
                //当前哈希桶的位置值不为null，也就是数组下标处有值，因为有值表示可能会发生冲突
                if ((e = oldTab[j]) != null) {
                    //把已经赋值之后的变量置为null，当然是为了好回收，释放内存
                    oldTab[j] = null;
                    //如果下标处的节点没有下一个元素
                    if (e.next == null)
                        //把该变量的值存入newCap中，e.hash & (newCap - 1)并不等于j
                        newTab[e.hash & (newCap - 1)] = e;
                    //该节点为红黑树结构，也就是存在哈希冲突，该哈希桶中有多个元素
                    else if (e instanceof TreeNode)
                        //把此树进行转移到newCap中
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                    else {
                      // 如果这个链表不止一个元素且不是一颗树
                        // 则分化成两个链表插入到新的桶中去
                        // 比如，假如原来容量为4，3、7、11、15这四个元素都在三号桶中
                        // 现在扩容到8，则3和11还是在三号桶，7和15要搬移到七号桶中去
                        // 也就是分化成了两个链表
                        Node<K,V> loHead = null, loTail = null; //低位链表
                        Node<K,V> hiHead = null, hiTail = null; //高位链表
                        Node<K,V> next;//指针：指向下一个节点
                        do {
                            next = e.next;
                            // (e.hash & oldCap) == 0的元素放在低位链表中
                            // 比如，3 & 4 == 0
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e; //往低位链表插入节点
                                else
                                    loTail.next = e;
                                loTail = e;
                            }
                            // (e.hash & oldCap) != 0的元素放在高位链表中
                            // 比如，7 & 4 != 0
                            else {
                                if (hiTail == null)
                                    hiHead = e; //往高位链表插入节点
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);//遍历结束
                        // 遍历完成分化成两个链表了
                        // 低位链表在新桶中的位置与旧桶一样，即3和11还在三号桶中
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;// 原索引放到bucket里
                        }
                        // 高位链表在新桶中的位置正好是原来的位置加上旧容量（即7和15搬移到七号桶了）
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead; // 原索引+oldCap放到bucket里
                        }
                    }
                }
            }
        }
        return newTab;
    }

    //如果插入元素后链表的长度大于等于8则判断是否需要树化。
    final void treeifyBin(Node<K,V>[] tab, int hash) {
        int n, index; //n:数组长度，index:索引
        Node<K,V> e;  //e是hash值和数组长度计算后，得到链表的首节点
        /** 如果数组为空或者数组长度小于树结构化的最小限制
    * MIN_TREEIFY_CAPACITY 默认值64，对于这个值可以理解为：如果元素数组长度小于这个值，没有必要去进行结构转换
    * 当一个数组位置上集中了多个键值对，那是因为这些key的hash值和数组长度取模之后结果相同。（并不是因为这些key的hash值相同）
    * 因为hash值相同的概率不高，所以可以通过扩容的方式，来使得最终这些key的hash值在和新的数组长度取模之后，拆分到多个数组位置上。
    */
        if (tab == null || (n = tab.length) < MIN_TREEIFY_CAPACITY) //如果桶的数量小于64，则进行扩容
            resize();
        // 如果元素数组长度已经大于等于了 MIN_TREEIFY_CAPACITY，那么就有必要进行结构转换了
      // 根据hash值和数组长度进行取模运算后，得到链表的首节点
        else if ((e = tab[index = (n - 1) & hash]) != null) {
            TreeNode<K,V> hd = null, tl = null;  //hd是树首节点，tl是树尾节点
            do {
                TreeNode<K,V> p = replacementTreeNode(e, null);//节点转换为树节点
                if (tl == null)//如果树尾节点为空说明没有根节点
                    hd = p;//附值给首节点，树的根节点
                else {// 尾节点不为空
                    p.prev = tl;// 将节点p的prev指向尾结点
                    tl.next = p;//再将尾结点的next指向该插入的节点
                }
                tl = p;// 把当前节点设为尾节点，值传递
            } while ((e = e.next) != null); //循环遍历完所有要转化的节点
            // 到目前为止 也只是把Node对象转换成了TreeNode对象，把单向链表转换成了双向链表
           // 把转换后的双向链表，替换原来位置上的单向链表
            // 如果进入过上面的循环，则从头节点开始树化
            if ((tab[index] = hd) != null)
                hd.treeify(tab); //红黑树的方法：树化成红黑树结构
        }
    }

    //将指定映射中的所有映射复制到此映射。
    public void putAll(Map<? extends K, ? extends V> m) {
        putMapEntries(m, true);
    }

    /**remove(Object key)  ——》  removeNode(int hash, Object key, Object value,boolean matchValue, boolean movable)*/

    //如果存在，则从此映射中删除指定键的映射。
    public V remove(Object key) {
        Node<K,V> e;
        return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;
    }

    //方法为final，不可被覆写，子类可以通过实现afterNodeRemoval方法来增加自己的处理逻辑
    /**第一参数为哈希值，第二个为key，第三个value，第四个为是为true的话，则表示删除它key对应的value，不删除key,第四个如果为false，则表示删除后，不移动节点**/
    final Node<K,V> removeNode(int hash, Object key, Object value,boolean matchValue, boolean movable) {
        //tab 哈希数组，p 当前节点，n 长度，index 索引
        Node<K,V>[] tab; Node<K,V> p; int n, index;
        //哈希数组不为null，且长度大于0，然后获得到要删除key的节点所在是数组下标位置
        if ((tab = table) != null && (n = tab.length) > 0 && (p = tab[index = (n - 1) & hash]) != null) {
            //node 存储要删除的节点，e 临时变量，k 当前节点的key，v 当前节点的value
            Node<K,V> node = null, e; K k; V v;
            //如果数组下标的节点正好是要删除的节点，把值赋给临时变量node
            if (p.hash == hash && ((k = p.key) == key || (key != null && key.equals(k))))
                node = p;
            /*
             * 到这一步说明首节点没有匹配上，那么检查下是否有next节点
             * 如果没有next节点，就说明该节点所在位置上没有发生hash碰撞, 就一个节点并且还没匹配上，也就没得删了，最终也就返回null了
             * 如果存在next节点，就说明该数组位置上发生了hash碰撞，此时可能存在一个链表，也可能是一颗红黑树
             */
            else if ((e = p.next) != null) {
                if (p instanceof TreeNode)//先判断是否为红黑树的节点
                    //遍历红黑树，找到该节点并返回
                    node = ((TreeNode<K,V>)p).getTreeNode(hash, key);
                else { //表示为链表节点，一样的遍历找到该节点
                    do {
                      // 如果e节点的键是否和key相等，e节点就是要删除的节点，赋值给node变量，调出循环
                        if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k)))) {
                            node = e;
                            break;
                        }
                        // 走到这里，说明e也没有匹配上
                        // 把当前节点p指向e，这一步是让p存储下一次循环里e的父节点，如果下一次e匹配上了，那么p就是node的父节点
                        /**注意，如果进入了链表中的遍历，那么此处的p不再是数组下标的节点，而是要删除结点的上一个结点**/
                        p = e;
                    } while ((e = e.next) != null);//终止循环
                }
            }

            //找到要删除的节点后，判断!matchValue，我们正常的remove删除，!matchValue都为true
            if (node != null && (!matchValue || (v = node.value) == value || (value != null && value.equals(v)))) {
                //如果删除的节点是红黑树结构，则去红黑树中删除
                if (node instanceof TreeNode)
                    ((TreeNode<K,V>)node).removeTreeNode(this, tab, movable);
                // 如果待删除的元素是第一个元素，则把第二个元素移到第一的位置
                else if (node == p)
                    tab[index] = node.next;
                else /**为链表结构，删除的节点在链表中，把要删除的下一个结点设为上一个结点的下一个节点**/
                    p.next = node.next;
                //修改计数器
                ++modCount;
                //长度减一
                --size;
                /**此方法在hashMap中是为了让子类去实现，主要是对删除结点后的链表关系进行处理**/
                afterNodeRemoval(node);//调用afterNodeRemoval方法，该方法HashMap没有任何实现逻辑，目的是为了让子类根据需要自行覆写
                //返回删除的节点
                return node;
            }
        }
        //返回null则表示没有该节点，删除失败
        return null;
    }
```


### 为什么要重写HashCode和equals方法？
面试官：你有没有重写过hashcode()方法？
面试者：没写过？
面试官：于是在通过一个问题确认：你在用HashMap的时候，key部分，有没有放过自定义对象？
面试者：放过。
面试官：于是两个问题就自相矛盾了。

由于在项目中经常会用到HashMap，所以在面试的时候一定会问到这个问题；你有没有重写过hashCode方法？你在使用HashMap的时候有没有重写hashCode和equals方法？你是怎么写的？
如果大家要在HashMap的key部分存放自定义的对象，一定要在这个对象里用自己的equals和hashCode方法覆盖掉Object里同名方法。
```java
class Key{
	private Integer id;
	public Key(Integer id){
		this.id = id;
	}
	public Integer getId(){
		return id;
	}
	public int hashCode(){
		return id.hashCode();
		//如果不重写hashCode,HashMap默认使用的是Key object的地址的hashCode()
	}

	//如果不重写equals默认比较对象Key的地址是否equals
	public boolean equals(Object o){
		if(o==null||!(o instanceof Key)){
			return false;
		}else{
			return this.getId().equals( (Key)o.getId() )
		}
	}
}
```



--------------------------------


#### 单线程的Redis为什么这么快？
（1）纯内存操作；
（2）单线程操作，避免了频繁的上下文切换
（3）才用了非阻塞I/O多路复用机制


我们的redis-client在操作的时候，会产生具有不同事件类型的socket。
在服务端，会有一段I/O多路复用程序，会将产生的socket置于队列中。然后事件分派器，一次区队列中取，装法到不同的事件处理器中。
单个线程------------->每个快递员
每个socket(i/o流)---->每个快递
socket的不同状态------>快递的送达地点
客户端的请求----------->客户送快递请求
服务端的请求---------->小明的经营方式
CPU的核数------------>一辆车

我们要仔细说一说I/O多路复用机制，一般人都不懂是什么意思。我来打个比方：小明在S城开了一家快递店，负责同城快送服务。小明因为资金限制，雇佣了一批快递员，然后小明发现资金不够了，只够买一辆车送快递。
经营方式一：
客户每送来一份快递，小明就让一个快递员盯着，然后快递员开车去送快递。
慢慢小明发现了这种经营方式存在的问题：

几十个快递员基本上时间都花在了抢车上了，大部分快递员都处在闲置状态，谁抢到了车，谁就能去送快递；

随着快递的增多，快递员也越来越多，小明发现快递店里越来越挤，没办法雇佣新的快递员了；

快递员之间的协调很花时间；

综合上述缺点，小明痛定思痛，提出了下面的经营方式。
经营方式二：
小明只雇佣一个快递员。然后呢，客户送来的快递，小明按送达地点标注好，然后依次放在一个地方。最后，那个快递员依次的去取快递，一次拿一个，然后开着车去送快递，送好了就回来拿下一个快递。

##HashSet
#####TreeSet和HashSet有什么区别？
HashSet使用的是HashMap来实现，而TreeSet使用的是TreeMap来实现的。

面试官:HashMap和TreeMap有什么区别吗？
HashMap是一个最常用的数据结构，他主要用于我们有通过固定值(key)获取内容的场景，时间复杂度可以最快优化到O(1),当然效果不好的时候时间复杂度是O(logN)或者O(N),虽然key值查找提高了速度，但是hashMap不能保证key的顺序。
所以这个时候TreeMap就出现了，虽然他的查找、删除、更新的时间复杂度都是O(logN),但是可以保证key的有序性。

面试官：那你和我说一下HashMap和TreeMap的底层实现有什么不同，才导致他们有这么大的差异？
这个原因主要是他们底层用的实现方式不同，HashMap使用的是数组和哈希的方式实现，巧妙的通过key的哈希路由到每一个数组存放内容（hashcode()&（length-1）），这个时候通过key获取value的时间复杂度就是O(1)，当然因为key的哈希可能碰撞，所以就需要针对碰撞的时候做处理，在JDK1.7里 HashMap里面每一个数组（桶）里面存放的其实是一个链表，key的哈希冲突以后会追加到链表上面，这个时候通过key获取value的时间复杂度就变成了O(n),如果碰撞的越多的时候岂不是查询的很慢？最后呢，为了优化这个时间复杂度，HashMap JDK1.8中，当一个key的碰撞次数超过了TREEIFY_THRESHOLD的时候把链表装换成红黑树，这样虽然插入的时候也增加了时间复杂度，但是对于频繁哈希碰撞的问题的查询效率有很大的提高，使得查询的时间复杂度变为了O(logN)
,说到红黑树就把HashMap和TreeMap联系到了一起，因为TreeMap底层实现就是红黑树

面试官：既然你说到了红黑树，那么我想问下为什么采用的是红黑树，而不是二叉搜索树呢？
嗯，通常情况下我们听到了二叉搜索树的时候以为它是平衡树，其实不是，它只是左子树的值小于根节点，右子树的值大于根节点。
如果构建根节点以后插入的数据是有序的，那么构造出来的二叉搜索树就不是平衡树，而是一个链表，那么它的时间复杂度就是O(n),如下图。

然后红黑树呢？
就是通过每个几点标色的方式，每次更新数据以后再进行平衡，以此来保证其查找效率。

面试官：既然你说到了这里，那么你再展开说明下红黑树是怎么做到的每次插入都平衡。
参考：https://www.bilibili.com/video/av23890827

参考2 https://mp.weixin.qq.com/s?__biz=MzIyNzc1ODQ0MQ==&mid=2247484628&idx=1&sn=3c8646e4bb9c38272f1fe07f1d0a2bf1&chksm=e85d1cd2df2a95c49e2e2dd511955ef5606d96f5ad53a3e8d4142ab72ce437d34a09b4961279&mpshare=1&scene=1&srcid=1023uNPv7z1sVpMsCCyYhqqk&sharer_sharetime=1571821169934&sharer_shareid=6e56c4c46055b28f308537063448710b%23rd

早上：
netty
上午：
并查集实现天花板的算法题：
并查集再做一道题目：
中午：
算法+JVM+Spark

晚上:
netty

进阶4包含红黑树
进阶5包含LRU
进阶6包含LFU+跳跃列表



#### 前面介绍了Map接口的实现类LinkedHashMap,LinkedHashMap存储的元素是有序的，可以保持元素的插入顺序，但是不能对元素进行自动排序。
再某些场景，如果再数据的存储过程中，能够自动对数据进行排序，将会极大提高编程效率，而Map接口有一个重要的实现类TreeMap，TreeMap可以实现存储元素的自动排序

> Java 的TreeMap实现了SortedMap接口，也就是说会按照key的大小顺序对Map中的元素进行排序，key大小的评判可以通过自然顺序(默认时升序)，也可以通过构造时传入的比较器（comparator）

TreeMap的底层实现时红黑树：红黑树的原理及代码实现：左神视频+B站视频https://www.bilibili.com/video/av23890827



## HashMap、LinkedHashMap和TreeMap用法和区别
》https://mp.weixin.qq.com/s?__biz=MzUzMzcyODE1MQ==&mid=2247483718&idx=1&sn=dcafc683052d09303dd4d241748a7288&chksm=fa9edfbecde956a8a73de758c29463c62a1f40ddea4071a1c7ef7c609874e42251636d137a7f&mpshare=1&scene=1&srcid=1023UxlFNF0hTlJwkEhxIIrH&sharer_sharetime=1571821058760&sharer_shareid=6e56c4c46055b28f308537063448710b%23rd


Map是一个接口，代表的是key-value键值对，Map中不能包含重复的key，一个key最多对应一个值，有一些Map的实现允许null值，有一些则不允许null值。

1 HashMap
基于哈希表的Map接口实现。除了未实现同步并允许null值，HashMap和HashTable大致一致，不过HashTable基本上已经废弃了，如果需要同步，可以使用ConcurrentHashMap作为更好的代替。

2 LinkedHashMap
LinkedHashMap也有HashMap的所有特性，它比HashMap多维护了一个双向链表，因此可以按照插入顺序从头部或者从尾部迭代，是有序的。
不过因为比HashMap多维护了一个双向链表，它的内存相比而言要比HashMap大，并且性能会差一些，但是如果需要考虑到元素插入的顺序的话，LinkedHashMap不失为一种好的选择。
LinkedHashMap的结构图如下：

3 TreeMap
与HashMap不同，TreeMap的底层就是一颗红黑树，它的containsKey,get,put,remove方法的时间复杂度都是log(n),并且按照key的自然顺序（或者制定排序）排列。
与LInkedHashMapbutong ,LinkedHashMap保证了元素时按照输入的顺序排序的。

用一张图表示HashMap、linkedHashMap，TreeMap之间的区别


-----------------------------------------------
## Redis内存
一、Redis内存被打满会发生什么？
1、通过配置文件配置内存
通过再Redis安装目录下面的redis.conf配置文件中添加以下配置设置内存大小
//设置redis最大占用内存大小为100m
maxmemory 100mb

2、通过命令修改
Redis支持运行时通过命令动态修改内存
//获取设置Redis能使用的最大内存大小
127.0.0.1：6379>config get maxmemory
//设置Redis最大占用内存大小为100M
127.0.0.1：6379>config set maxmemory
如果不设置最大内存大小或者设置最大内存大小为0，在64位操作系统下不限制内存大小。

二、Redis的内存淘汰
既然可以设置Redis最大占用内存大小，那么配置的内存就有用完的时候。那在内存用完的时候，还继续往redis里面添加数据不就没有内存可用了吗？
实际上redis定义了几种策略来处理这种情况：
noeviction(默认策略)：对于写请求不再提供服务，直接返回错误
allkeys-lru:重说有key中使用LRU算法进行淘汰
volatile-lru:从设置了过期时间的key中使用LRU算法进行淘汰

设置淘汰策略
127.0.0.1> config set maxmemory-policy allkeys-lru

三、LRU算法（）
什么是LRU?
LRU(Least Recently Used)即最近最少使用算法。Redis3.0中增加了 近似LRU算法
其核心思想是：如果一个数据在最近一段时间没有被用到，那么将来被使用到的可能性也很小，所以就可以被淘汰掉。
LRU算法基于一种假设，长期不被使用的数据，在未来被用到的几率也不大。因此，当数据所占内存达到一定的阈值时，我们要移除掉最近最少使用的数据。


LUR算法的代码实现：
```java
//LRU算法的核心：就在于使用了一种数据结构--哈希链表
//LRU可以用自己的代码实现；也可以使用Java中的LinkedHashMap来实现
//因为LinkedHashMap已经对哈希家链表做了很好的实现，但为了加深印象，我们还是自己写代码简单实现一下吧：
//需要注意的是，下面这段代码不是线程安全的，要想做到线程安全，需要加上synchronized修饰符号。

public LRUCache{
	public int capacity;
	public LinkedHashMap<Integer,Integer> cacheMap;
}

```

```java
//使用LinkedHashMap来实现
public LRUCache{
	public int capacity;
	public LinkedHashMap<Integer,Integer> cacheMap;

	public LRUCache(int capacity){
		this.capacity = capacity;
		this.cacheMap = new LinkedHashMap<Integer,Integer>(16,0.75F,true){
			@Override
			protected boolean removeEldestEntry(Map.Entry<Integer,Integer>eldest){
				if(cacheMap.size()==capacity+1)
					return true;
				else
					return false;

			}
		};
	}

	public int get(int key){
		Integer res = cacheMap.get(key);
		if(res==null){
			return -1;
		}
		return res;
	}

	public void put(int key,int value){
		cacheMap.put(key,value);
	}
}

```


哈希表是由若干个key-value所组成。在逻辑上，这些key-value是无所谓排列顺序的，谁先谁后都一样。
如下图。

在哈希链表中，这些key-value不再是彼此无关的存在，而是被一个链条串起来，每一个key-value都具有它的前驱key-value、后继key-value,
如下图。
这样一来原本无序的哈希表拥有了固定的排列顺序。

- LinkedHashMap大体的LRU架子都为我们搭好了。那我们怎么去基于LinkedHashMap实现LRU呢。先别慌，我们先看看Mysql-jdbc中的LruCache是怎么实现的。
```java
public class LRUCache extends LinkedHashMap<Object,Object>{
	protected int maxElements;
	public LRUCache(int maxSize){
		super(maxSize, 0.75F, true);
		this.maxElements = maxSize;
	}

	protected boolean removeEledestEntry(Entry<Object,Object>eldest){
		return this.size()>this.maxElements;

	}
}
//其实我们只要把accessOrder设置为ture,重写removeEldestEntry(eldest)即可。我们在removeEldestEntry(eldest)加上什么时候执行LRU操作的逻辑，比如map里面的元素的数量超过制定的大小，开始删除最近最少使用的元素，为后序新增的元素腾出位置来。

```


四、LFU算法



五、Redis为什么不使用准确的LRU算法而是使用近似LRU算法。

10月28日 任务：
LRU 自己实现代码
UnionFind 代码一题
左神滑动窗口题目

中午：
Spark:
JVM
算法课程：红黑树


下午
netty张龙

傍晚：
面试题

晚上：
面试题


_----------------------------------------------\

### 线程安全的List
面试官：List(ArrayList)是线程安全的么？怎么保证它的线程安全性呢？或者有什么替代方案？
面试者：
可以使用vector 或者java.util.Collections.SynchronizedList
它能把所有List接口的实现类转换成线程安全的List.
比Vector有更好的扩展性和兼容性，SynchronizedList的构造方法。
```java
final List<E> list;

SynchronizedList(List<E> list) {
    super(list);
    this.list = list;
}
```
//synchronizedList的部分源码如下：
```java
public E get(int index) {
    synchronized (mutex) {return list.get(index);}
}
public E set(int index, E element) {
    synchronized (mutex) {return list.set(index, element);}
}
public void add(int index, E element) {
    synchronized (mutex) {list.add(index, element);}
}
public E remove(int index) {
    synchronized (mutex) {return list.remove(index);}
}
```
面试官：它所有的方法都是带有同步对象锁，和Vector一样，它不是性能最优的。如果在读多写少的情况下，synchronizedList这种集合的性能非常差，有没有更适合的方案？
面试者：
java.util.concurrent.CopyOnWriteArrayList
java.util.concurrent.CopyOnWriteArraySet
CopyOnWrite结合类也就这两个
CopyOnWrite(简称COW):即复制再写入，就是在添加元素的时候，先把List列表先复制一份，再添加新的元素。
其add方法源码：
```java
//添加元素时，先加锁，再进行复制替换操作，最后再释放锁。
public boolean add(E e) {
    // 加锁
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        // 获取原始集合
        Object[] elements = getArray();
        int len = elements.length;

        // 复制一个新集合
        Object[] newElements = Arrays.copyOf(elements, len + 1);
        newElements[len] = e;

        // 替换原始集合为新集合
        setArray(newElements);
        return true;
    } finally {
        // 释放锁
        lock.unlock();
    }
}
```

其get方法源码
```java
//可以看到，获取元素并没有加锁。
//这样的好处是，在高并发情况下，读取元素时就不用加锁，写数据时才加锁，大大提升了读取性能
private E get(Object[] a, int index) {
    return (E) a[index];
}

public E get(int index) {
    return get(getArray(), index);
}
```
CopyOnWriteArraySet

CopyOnWriteArraySet逻辑就更简单了，就是使用 CopyOnWriteArrayList 的 addIfAbsent 方法来去重的，添加元素的时候判断对象是否已经存在，不存在才添加进集合
```java
public boolean addIfAbsent(E e) {
    Object[] snapshot = getArray();
    return indexOf(e, snapshot, 0, snapshot.length) >= 0 ? false :
        addIfAbsent(e, snapshot);
}
```
这两种并发集合，虽然牛逼，但是只适合读多写少的情况，如果写多读少，使用这个就没有意义了，因为每次读写操作都要进行集合内存复制，性能开销很大，如果集合很大，很容易造成内存溢出。

下次面试官问你线程安全的 List，你可以从 Vector > SynchronizedList > CopyOnWriteArrayList 这样的顺序依次说上来，这样才有带入感，也能体现你对知识点的掌握程度。

--------------------------------------------
### WeakHashMap
WeakHashMap从名字上看与HashMap相关，但是与HashMap有着很大的差别，翻译成中文后表示弱HashMap,俗称缓存HashMap

WeakHashMap是一个很特殊的成员，它的特殊之处在于WeakHashMap里的元素可能会被GC自动删除，即使程序员没有显示的调用remove()和Clear()
即，当向WeakHashMap中添加元素的时候，再次遍历获取元素，可能发现它已经不见了，
例如：
```java
public class TestWeakHashMap {
    public static void main(String[] args){
        Map weakHashMap = new WeakHashMap();
        //向weakHashMap中添加4个元素
        for(int i=0;i < 3;i++){
            weakHashMap.put("key-"+i,"value-"+i);
        }
        //输出添加的元素
        System.out.println("数组长度："+weakHashMap.size() + "，输出结果：" + weakHashMap);
        //主动触发一次GC
        System.gc();
        //再输出添加的元素
        System.out.println("数组长度："+weakHashMap.size() + "，输出结果：" + weakHashMap);
    }
}
//输出结果：
//数组长度：3，输出结果：{key-2=value-2, key-1=value-1, key-0=value-0}
//数组长度：3，输出结果：{}
//当主动调用GC回收器的时候，再次查询WeakHashMap里面的数据的时候，内容为空
```
- 对象引用的介绍
对象的引用分为四种级别：由高到低依次为 强引用、软引用、弱引用和虚引用
这样可以灵活控制对象的生命周期
对比
引用类型 被垃圾回收时间  用途
强引用   从来不会       对象的一般状态
软引用   在内存不足时候回收 对象缓存
弱引用   在垃圾回收时候     对象缓存
虚引用

WeakHashMap跟普通的HashMap不同，在存储数据时候，key被设置为弱引用类型，而弱引用类型在java中，可能随时被jvm的gc回收，所以再次通过获取对象时候，可能得到空值，而value是在访问数组内容的时候，进行清除.
可能很多人觉得这样做很奇葩，其实不然，WeekHashMap 的这个特点特别适用于需要缓存的场景。

在缓存场景下，由于系统内存是有限的，不能缓存所有对象，可以使用 WeekHashMap 进行缓存对象，即使缓存丢失，也可以通过重新计算得到，不会造成系统错误。


---------------------------------------------
零拷贝
面试官问零拷贝？
从用户态谈到内核态、socket谈到FileChannel、从NIO谈到Netty,从直接内存到CompositeBuf,从FileRegion到wrap......

- 首先考虑一个场景
从一个文件中读取数据并降数据传到另一台服务器上？
```java
//伪代码
File.read(file,buf,len);
Sokcet.sent(socket,buf,len);
```
上述的代码中涉及几次数据拷贝？
答：这种方式一共涉及四次数据拷贝，

*面试官*：知道用户态和内核态的区别么？

- 非零拷贝的过程：
1、应用程序中调用read()方法，这里会涉及到以此上下文切换（用户态->内核态），底层采用DMA(direct memory access)读取磁盘的文件，并把内容存储到内核地址空间的 read Buffer

2、由于应用程序无法访问内核地址空间的数据，如果应用程序需要操作这些数据，得把这些内容从 read Buffer拷贝到 Application buffer(用户缓冲区)。read()调用的返回引发了一次上下文切换（内核态->用户态），现在数据已经被拷贝到了Application Buffer。如果有需要，可以操作修改这些内容。

3、我们最终目的是把这个文件内容通过socket传到另一个服务中，调用Socket的send()方法，又涉及到以此上下文切换（用户态->内核态），同时，文件内容被进行了第三次拷贝。这次的缓冲区与目标Socket相关联，与ReadBuffer无关。

4、send()调用返回，引发了第四次的上下文切换，同时进行第四次拷贝，DMA把数据从目标Socket相关的缓冲区socket Buffer传到协议引擎进行发送。

- 零拷贝过程



***
## JVM发生内存溢出的8种原因、以及解决办法

1、Java堆空间发生内存溢出原因
    造成原因：
    - 无法在Java堆中分配对象
    - 应用程序无意保存了对象引用，对象无法被GC回收
    - 吞吐量突然增加
    解决方案：
    - 使用-Xmx增加堆大小
    - 修复应用程序中的内存泄露

2、GC开销超过限制
    造成原因：
    - Java进程98%的时间在进行垃圾回收，恢复了不到2%的堆空间，连续5次后

    解决方案：
    - 使用-Xmx 增加堆大小
    - 使用 -XX:-UseGCOverheadLimit取消GC开销限制
    - 修复应用程序中的内存泄露

3、请求的数组大小超过JVM虚拟机限制
    造成原因：
    - 应用程序试图分配一个超过堆大小的数组
    解决方案：
    - 使用-Xmx增加堆大小
    - 修复应用程序中分配巨大数组的Bug

4、Perm gen空间
    造成原因：
    Perm gen空间包含：
        + 类的名字、字段、方法
        + 与类相关的对象数组和类型数组
        + JIT编译器优化
    当Perm gen空间用尽时，将抛出异常。

    解决办法：
    - 使用-XX:MaxPermSize 增加Permgen大小
    - 不重启应用程序可能会导致此问题，重启JVM解决。


5、Metaspace
    造成原因：
    - 从java 8开始Perm gen改成了Metaspace,在本机内存中分配class 元数据（称为metaspace）。如果metaspace耗尽，这抛出异常。

    解决方案：
    - 通过命令行设置 -XX:MaxMetaSpaceSize增加metaspace大小
    - 减小Java堆内存大小，为MetaSpace提供更多的可用空间
    - 为服务器分配更多的内存

6、无法新建本机线程
    造成原因
    内存不足，无法创建线程。由于线程在本机内存中创建，报告这个错误表明本机内存空间不足

    解决方案
    - 为机器分配更多的内存
    - 减少Java堆空间
    - 使用-Xss减小线程堆栈大小
    - 修复应用程序中线程泄露


****
## synchronized的基本使用
synchronized的作用主要有三个：
- 确保线程互斥访问同步代码
- 保证共享变量的修改能够及时看见
- 有效解决重排序问题

从语法上讲，Synchonize总共有三种用法：
修饰普通方法
修饰静态方法：对静态方法的同步本质上是对类的同步(静态方法本质上是属于类的方法，而不是对象上的方法)
修饰代码块

#### JVM实现建议补充 sync偏向锁、轻量锁、重量锁，面试重点、面试常考：Synchronized 有几种用法？



***
1、为什么要用锁？
2、锁实现的基本原理？
    + volatile
    + snchronized
    + CAS
3、Java中的锁实现
    + 队列同步器(AQS)
4、锁的使用用例
    + ConcurrentHashMap

1、为什么要用锁？
锁-是为了解决并发操作引起的脏读、数据不一致问题。

2、锁实现的基本原理？
    - volatile实现的基本原理
        + java编程语言允许线程访问共享变量，为了确保共享变量能够被准备和一致性的更新，线程应该确保通过拍他锁单独获得这个变量，java语言提供了Volatile，在某些情况下比锁要更加方便
        + volatile在多处理器开发中保证了共享变量的"可见性"，可见性的意思是当一个线程修改了一个共享变量的时候，另外一个线程也能读到这个修改的值。
    - 结论：如果Volatile修饰符使用恰当的话，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文切换和调度。

    - synchronized实现的基本原理
        + synchronize通过锁机制实现同步
        + synchronized实现同步的基础：Java中的每一个对象都可以作为锁。
        + 具体表现为以下3种形式。
        对于普通同步方法，锁是当前实例对象。
        对于静态同步方法，锁是当前类的Class对象。
        对于同步方法块，锁是Synchonized括号里配置的对象。
        当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁

        + synchronized实现原理
            + syncrhonized是基于Monitor来实现同步的
            + Monitor从两个方面来支持线程之间同步：
                + 互斥性
                + 协作
            1、 Java使用对象锁（使用synchronized获得对象锁）保证工作在共享的数据集上的线程互斥执行。
            2、使用notify/notifyAll/wait 方法来协同不同线程之间的工作
            3、Class 和Object都关联了一个Monitor（监视器锁）

        + Monitor的工作机理（如下图）
            - 线程进入同步方法中
            - 为了继续执行临界区代码，线程必须获取Monitor锁。如果获取锁成功，将成为该监视者对象的拥有者。任意时刻，监视者对象只属于一个活动线程（The Owner）
            - 拥有监视者对象的线程可以调用wait()进入等待集合（Wait Set），同时释放监视锁，进入等待状态。
            - 其他线程调用 notify()/notifyAll()接口唤醒等待集合中的线程，这些等待的线程需要重新获取见识说后才能执行wait()之后的代码。
            - 同步方法执行完毕了，线程退出临界区，并释放监视锁。
            如图：

        + Synchronized具体实现
            1、同步代码块采用monitorenter、monitorexit指令显示的实现
            2、同步方法则使用ACC_SYNCHRONIZED标记符隐式的实现

```java
    public class SynchronizedTest{
        public synchronized void method(){
            System.out.println("hello world");
        }
        public void method2(){
            synchronized(this){
                System.out.println("hello world");
            }
        }
    }
```
如图：

- monitorenter
    + 每一个对象都有一个monitor,一个monitor只能被一个线程拥有。当一个线程执行到monitorenter指令时会尝试获取相应对象的monitor,获取规则如下。
        * 如果monitor的进入数为0，则该线程可以进入monitor,并将monitor进入数设置为1，该线程即为monitor的拥有者。
        * 如果当前线程已经拥有该monitor，只能重新进入，这进入monitor的进入数加1，所以synchronized关键字实现的锁时可重入的锁。
        * 如果monitor已被其他线程拥有，则当前线程进入阻塞状态，直到monitor的进入数为0，在重新尝试获取monitor。
    +
- monitorexit
    + 只有拥有响应对象monitor的线程才能执行monitorexit指令，每执行一次该指令monitor进入数减1，当进入数为0时，当前线程释放monitor,此时其他阻塞的线程可以尝试获取该monitor。
- monitor锁存放的位置
    + 锁标记存放在Java对象头的Mark Word中
    + 如图
- syncrhonized的锁优化






------算法任务：
1、Sliding Window  10
2、Dynamic Programming 10
3、贪心算法 10
4、深度优先 10
5、广度优先搜索 10
6、设计 10
7、二叉搜索树 10



# JVM案例1

### 生产上JVM案例分析
- 项目介绍
每日百亿数据的实时分析引擎
ATM及各种设备的日志插入到数据库40w台设备，每台设备平均500次/日，平均一次涉及钞箱的交易为4次
4*500*40w = 8亿条数据每日。

不停的从数据库中提取大量的数据到自己的JVM中
一套分布式运行的系统，所以在生产环境部署了多台 机器，每台机器大概每分钟负责执行100次数据提取和计算

- 多久执行一次young GC
一次计算大概60次数据提取和计算(1分钟)
每次取1w条左右的数据到内存中来计算，60w  6台机器的一个集群 4C 8G


每分钟会执行60次数据计算任务，每次是1万条数据需要计算10秒
jvm给了4G,其中新生代和老年代分别是1.5G的内存
新生代是按照 8：1：1来分配；eden 1.2G  survivor1 100MB  survivor2 100MB
每条数据平均包含了20个字段50*20=1000Byte = 1kb
那么每次计算任务1万条数据对应的10MB大小

基本上按照这个内存大小而言，每次执行一个计算任务，就会在Eden区里分配10MB左右的对象，那么一分钟大概对应60次计算任务，其实基本上二分钟过后，Eden区域就全是对象，基本上就全满了。

所以新生代里的Eden区，基本上1分钟左右就迅速填满了。

- 触发MinorGC时候会有多少对象进入老年代？










class Solution {
    /**
    * 1. window中left不动，right移动，right停止移动的条件 t字符串中的字符全部被left与right之间包含
    * 2. window中right不动，left往右移动
    * 3. left移动的停止条件：right-left+1>tlen
    * 4. 当right移动到s字符串的末尾，整个遍历结束
    */
    public String minWindow(String s, String t) {
        String res = null;
        if(s==null||t==null) return res;
        HashMap<Character,Integer> map = new HashMap<>();

        int slen = s.length();
        int tlen = t.length();

        //将t字符串中的字符保存入HashMap
        for(int i=0;i<tlen;i++){
            if(map.containsKey(t.charAt(i))){
                map.put(t.charAt(i),map.get(t.charAt(i))+1);
            }else
                map.put(t.charAt(i),1);
        }

        int left =0;
        int right =0;
        int start =0;
        int end = 0;
        int len =0;
        int strlen = 0;
        int min = 0;

        while(right<slen){
            char key = s.charAt(right);

            //left不动，right移动
            if(map.containsKey(key)){//t中包含right指向的val
                map.put(key,map.get(key)-1);
                strlen++;
                if(map.get(key)>=0){
                    len++;
                }
            }else{
                strlen++;
            }

            //right不动，left移动
            if(len == tlen){
                if(strlen<min){
                    start = left;
                    end = right;
                }
                while(right-left+1>tlen){

                    if(strlen<min){
                        start = left;
                        end = right;
                    }

                    //移动left
                    key = s.charAt(left);
                    if(map.containsKey(key)){
                        map.put(key,map.get(key)+1);

                        if(map.get(key)>0){
                            len--;
                        }
                        strlen--;
                        left++;
                    }else{
                        strlen--;
                        left++;
                    }
                }
            }


            right++;
        }
        return s.substring(left-1,right);
    }
}




*********************java的堆外内存与Flink的堆外内存与netty的堆外内存


*********************JVM调优***************
-XX:NewSize=10485760 -XX:MaxNewSize=10485760 -XX:InitialHeapSize=20971520 -XX:MaxHeapSize=20971520 -XX:SurvivorRatio=8  -XX:MaxTenuringThreshold=15 -XX:PretenureSizeThreshold=10485760 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc.log










